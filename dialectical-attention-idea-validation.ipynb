{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5971518,"sourceType":"datasetVersion","datasetId":3423654},{"sourceId":10476716,"sourceType":"datasetVersion","datasetId":6487234},{"sourceId":11894306,"sourceType":"datasetVersion","datasetId":7476409}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nfrom collections import defaultdict\nimport time\nimport warnings\nimport pandas as pd\nimport re\nimport math\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# CONFIGURATION - SCALABLE\n# ============================================================================\nCONFIG = {\n    # Scalable Architecture Parameters\n    'd_model': 256,\n    'n_heads': 8,\n    'n_layers': 4,\n    'n_minds': 2,\n    'd_ff': 512,\n    'dropout': 0.1,\n    \n    # Dialectical Parameters\n    'bottleneck_ratio': 0.25,\n    \n    # Training\n    'vocab_size': 5000,\n    'max_seq_len': 128,\n    'batch_size': 8,\n    'num_epochs': 20,\n    'learning_rate': 0.0005,\n    \n    # Data\n    'train_samples': 600,\n    'test_samples': 150,\n    \n    # Device\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n}\n\n# ============================================================================\n# UTILITY FUNCTIONS\n# ============================================================================\n\ndef print_separator(title=\"\", char=\"=\", width=70):\n    if title:\n        padding = (width - len(title) - 2) // 2\n        print(f\"\\n{char * padding} {title} {char * padding}\")\n    else:\n        print(char * width)\n\ndef print_config():\n    print_separator(\"SCALABLE DIALECTICAL TRANSFORMER CONFIG\")\n    for key, value in CONFIG.items():\n        print(f\"  {key:25s}: {value}\")\n    print_separator()\n\ndef count_parameters(model):\n    total = sum(p.numel() for p in model.parameters())\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return total, trainable\n\n# ============================================================================\n# TOKENIZER\n# ============================================================================\n\nclass SimpleTokenizer:\n    def __init__(self, vocab_size=5000):\n        self.vocab_size = vocab_size\n        self.word2idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}\n        self.idx2word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}\n        self.next_idx = 4\n        \n    def fit(self, texts):\n        word_counts = defaultdict(int)\n        for text in texts:\n            for word in self._tokenize(text):\n                word_counts[word] += 1\n        \n        sorted_words = sorted(word_counts.items(), key=lambda x: -x[1])\n        for word, _ in sorted_words[:self.vocab_size - 4]:\n            if word not in self.word2idx:\n                self.word2idx[word] = self.next_idx\n                self.idx2word[self.next_idx] = word\n                self.next_idx += 1\n                \n    def _tokenize(self, text):\n        text = text.lower()\n        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n        return text.split()\n    \n    def encode(self, text, max_len):\n        words = self._tokenize(text)\n        indices = [self.word2idx.get(w, 1) for w in words]\n        if len(indices) < max_len:\n            indices = indices + [0] * (max_len - len(indices))\n        else:\n            indices = indices[:max_len]\n        return indices\n\n# ============================================================================\n# DIALECTICAL ATTENTION - CORE INNOVATION\n# ============================================================================\n\nclass DialecticalAttention(nn.Module):\n    \"\"\"\n    Multi-head attention split into \"minds\" that debate.\n    \n    Standard: All heads work together\n    Dialectical: Heads split into minds ‚Üí debate ‚Üí merge\n    \"\"\"\n    def __init__(self, d_model, n_heads, n_minds=2, dropout=0.1):\n        super().__init__()\n        assert n_heads % n_minds == 0, \"n_heads must be divisible by n_minds\"\n        \n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.n_minds = n_minds\n        self.heads_per_mind = n_heads // n_minds\n        self.d_head = d_model // n_heads\n        \n        # Each mind has Q, K, V projections\n        self.mind_qkv = nn.ModuleList([\n            nn.Linear(d_model, 3 * d_model // n_minds, bias=False)\n            for _ in range(n_minds)\n        ])\n        \n        # Output projection\n        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n        \n        # Mind-specific biases (creates perspectives)\n        self.mind_bias = nn.Parameter(torch.randn(n_minds, d_model) * 0.02)\n        \n        # Cross-mind debate\n        bottleneck_dim = int(d_model * CONFIG['bottleneck_ratio'])\n        self.thought_compress = nn.Linear(d_model // n_minds, bottleneck_dim)\n        self.thought_expand = nn.Linear(bottleneck_dim * n_minds, d_model // n_minds)\n        \n        # Debate gate\n        self.debate_gate = nn.Sequential(\n            nn.Linear(d_model // n_minds * 2, d_model // n_minds),\n            nn.Sigmoid()\n        )\n        \n        self.dropout = nn.Dropout(dropout)\n        self.scale = self.d_head ** -0.5\n        \n    def forward(self, x, mask=None, return_mind_outputs=False):\n        batch_size, seq_len, _ = x.shape\n        \n        mind_outputs = []\n        mind_thoughts = []\n        \n        # Phase 1: Each mind processes independently\n        for mind_idx in range(self.n_minds):\n            x_mind = x + self.mind_bias[mind_idx].unsqueeze(0).unsqueeze(0)\n            \n            qkv = self.mind_qkv[mind_idx](x_mind)\n            qkv = qkv.reshape(batch_size, seq_len, 3, self.heads_per_mind, self.d_head)\n            qkv = qkv.permute(2, 0, 3, 1, 4)\n            q, k, v = qkv[0], qkv[1], qkv[2]\n            \n            attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n            \n            if mask is not None:\n                attn = attn.masked_fill(mask == 0, float('-inf'))\n            \n            attn = F.softmax(attn, dim=-1)\n            attn = self.dropout(attn)\n            \n            out = torch.matmul(attn, v)\n            out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n            mind_outputs.append(out)\n            \n            thought = self.thought_compress(out)\n            mind_thoughts.append(thought)\n        \n        # Phase 2: Cross-mind debate\n        all_thoughts = torch.cat(mind_thoughts, dim=-1)\n        shared_insight = self.thought_expand(all_thoughts)\n        \n        # Phase 3: Incorporate shared insight\n        debated_outputs = []\n        for mind_idx, out in enumerate(mind_outputs):\n            gate_input = torch.cat([out, shared_insight], dim=-1)\n            gate = self.debate_gate(gate_input)\n            debated = out * (1 - gate) + shared_insight * gate\n            debated_outputs.append(debated)\n        \n        # Combine\n        combined = torch.cat(debated_outputs, dim=-1)\n        output = self.out_proj(combined)\n        \n        if return_mind_outputs:\n            return output, mind_outputs, mind_thoughts\n        return output\n\nclass DialecticalTransformerBlock(nn.Module):\n    \"\"\"Transformer block with dialectical attention.\"\"\"\n    def __init__(self, d_model, n_heads, d_ff, n_minds=2, dropout=0.1):\n        super().__init__()\n        \n        self.attn_norm = nn.LayerNorm(d_model)\n        self.ffn_norm = nn.LayerNorm(d_model)\n        \n        self.attention = DialecticalAttention(d_model, n_heads, n_minds, dropout)\n        \n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout)\n        )\n        \n    def forward(self, x, mask=None, return_details=False):\n        normed = self.attn_norm(x)\n        \n        if return_details:\n            attn_out, mind_outputs, mind_thoughts = self.attention(\n                normed, mask, return_mind_outputs=True\n            )\n        else:\n            attn_out = self.attention(normed, mask)\n            mind_outputs, mind_thoughts = None, None\n        \n        x = x + attn_out\n        \n        normed = self.ffn_norm(x)\n        ffn_out = self.ffn(normed)\n        x = x + ffn_out\n        \n        if return_details:\n            return x, mind_outputs, mind_thoughts\n        return x\n\n# ============================================================================\n# FULL DIALECTICAL TRANSFORMER\n# ============================================================================\n\nclass DialecticalTransformer(nn.Module):\n    \"\"\"Scalable Dialectical Transformer.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.name = \"DialecticalTransformer\"\n        \n        d_model = CONFIG['d_model']\n        n_heads = CONFIG['n_heads']\n        n_layers = CONFIG['n_layers']\n        n_minds = CONFIG['n_minds']\n        d_ff = CONFIG['d_ff']\n        dropout = CONFIG['dropout']\n        \n        self.token_embed = nn.Embedding(CONFIG['vocab_size'], d_model)\n        self.pos_embed = nn.Embedding(CONFIG['max_seq_len'], d_model)\n        \n        self.layers = nn.ModuleList([\n            DialecticalTransformerBlock(d_model, n_heads, d_ff, n_minds, dropout)\n            for _ in range(n_layers)\n        ])\n        \n        self.final_norm = nn.LayerNorm(d_model)\n        \n        self.head = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model, 1)\n        )\n        \n        self.apply(self._init_weights)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            \n    def forward(self, x, return_details=False):\n        batch_size, seq_len = x.shape\n        \n        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n        h = self.token_embed(x) + self.pos_embed(positions)\n        \n        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device))\n        mask = mask.unsqueeze(0).unsqueeze(0)\n        \n        all_mind_outputs = []\n        all_mind_thoughts = []\n        \n        for layer in self.layers:\n            if return_details:\n                h, mind_outputs, mind_thoughts = layer(h, mask, return_details=True)\n                all_mind_outputs.append(mind_outputs)\n                all_mind_thoughts.append(mind_thoughts)\n            else:\n                h = layer(h, mask)\n        \n        h = self.final_norm(h)\n        output = self.head(h.mean(dim=1))\n        \n        if return_details:\n            return output, all_mind_outputs, all_mind_thoughts\n        return output, []\n\n# ============================================================================\n# BASELINE MODELS\n# ============================================================================\n\nclass StandardTransformer(nn.Module):\n    \"\"\"Standard transformer without dialectical mechanisms.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.name = \"StandardTransformer\"\n        \n        d_model = CONFIG['d_model']\n        n_heads = CONFIG['n_heads']\n        n_layers = CONFIG['n_layers']\n        \n        self.token_embed = nn.Embedding(CONFIG['vocab_size'], d_model)\n        self.pos_embed = nn.Embedding(CONFIG['max_seq_len'], d_model)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=CONFIG['d_ff'],\n            dropout=CONFIG['dropout'],\n            batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        \n        self.final_norm = nn.LayerNorm(d_model)\n        \n        self.head = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.GELU(),\n            nn.Dropout(CONFIG['dropout']),\n            nn.Linear(d_model, 1)\n        )\n        \n    def forward(self, x, return_details=False):\n        batch_size, seq_len = x.shape\n        \n        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n        h = self.token_embed(x) + self.pos_embed(positions)\n        \n        mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(x.device)\n        \n        h = self.encoder(h, mask=mask)\n        h = self.final_norm(h)\n        \n        output = self.head(h.mean(dim=1))\n        \n        return output, []\n\nclass PersonaTransformer(nn.Module):\n    \"\"\"Previous winner adapted to same scale.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.name = \"PersonaTransformer\"\n        \n        d_model = CONFIG['d_model']\n        n_minds = CONFIG['n_minds']\n        \n        self.token_embed = nn.Embedding(CONFIG['vocab_size'], d_model)\n        self.pos_embed = nn.Embedding(CONFIG['max_seq_len'], d_model)\n        \n        self.shared_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=CONFIG['n_heads'],\n            dim_feedforward=CONFIG['d_ff'],\n            dropout=CONFIG['dropout'],\n            batch_first=True\n        )\n        \n        self.personas = nn.Parameter(torch.randn(n_minds, d_model) * 0.1)\n        nn.init.orthogonal_(self.personas)\n        \n        self.persona_strength = nn.Parameter(torch.ones(n_minds) * 0.3)\n        self.hidden_update = nn.GRUCell(d_model, d_model)\n        \n        self.final_norm = nn.LayerNorm(d_model)\n        \n        self.head = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.GELU(),\n            nn.Dropout(CONFIG['dropout']),\n            nn.Linear(d_model, 1)\n        )\n        \n    def forward(self, x, return_details=False):\n        batch_size, seq_len = x.shape\n        \n        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n        h = self.token_embed(x) + self.pos_embed(positions)\n        \n        hidden = torch.zeros(batch_size * seq_len, CONFIG['d_model']).to(x.device)\n        \n        thoughts = []\n        \n        for i in range(CONFIG['n_minds']):\n            persona = self.personas[i] * torch.sigmoid(self.persona_strength[i])\n            h_persona = h + persona.unsqueeze(0).unsqueeze(0)\n            \n            flat_h = h_persona.reshape(-1, CONFIG['d_model'])\n            hidden = self.hidden_update(flat_h, hidden)\n            \n            seq_h = hidden.reshape(batch_size, seq_len, CONFIG['d_model'])\n            processed = self.shared_layer(seq_h)\n            \n            thoughts.append(processed.mean(dim=1).detach())\n            \n            h = processed\n            hidden = processed.reshape(-1, CONFIG['d_model'])\n        \n        h = self.final_norm(h)\n        output = self.head(h.mean(dim=1))\n        \n        return output, thoughts\n\n# ============================================================================\n# METRICS\n# ============================================================================\n\ndef compute_mind_diversity(mind_outputs):\n    if mind_outputs is None or len(mind_outputs) == 0:\n        return 0.0\n    \n    total_sim = 0\n    count = 0\n    \n    for layer_minds in mind_outputs:\n        if layer_minds is None:\n            continue\n        for i in range(len(layer_minds)):\n            for j in range(i + 1, len(layer_minds)):\n                m1 = layer_minds[i].reshape(layer_minds[i].size(0), -1)\n                m2 = layer_minds[j].reshape(layer_minds[j].size(0), -1)\n                sim = F.cosine_similarity(m1, m2).mean()\n                total_sim += sim.item()\n                count += 1\n    \n    if count == 0:\n        return 0.0\n    \n    return 1 - (total_sim / count)\n\ndef compute_convergence(mind_thoughts):\n    if mind_thoughts is None or len(mind_thoughts) < 2:\n        return 0.0\n    \n    first_layer = mind_thoughts[0]\n    last_layer = mind_thoughts[-1]\n    \n    if first_layer is None or last_layer is None:\n        return 0.0\n    \n    first_sim = 0\n    last_sim = 0\n    count = 0\n    \n    for i in range(len(first_layer)):\n        for j in range(i + 1, len(first_layer)):\n            f1 = first_layer[i].reshape(first_layer[i].size(0), -1)\n            f2 = first_layer[j].reshape(first_layer[j].size(0), -1)\n            first_sim += F.cosine_similarity(f1, f2).mean().item()\n            \n            l1 = last_layer[i].reshape(last_layer[i].size(0), -1)\n            l2 = last_layer[j].reshape(last_layer[j].size(0), -1)\n            last_sim += F.cosine_similarity(l1, l2).mean().item()\n            count += 1\n    \n    if count == 0:\n        return 0.0\n    \n    return (last_sim - first_sim) / count\n\ndef compute_metrics(pred, target, mind_outputs, mind_thoughts):\n    metrics = {}\n    metrics['mse'] = F.mse_loss(pred, target).item()\n    metrics['mae'] = F.l1_loss(pred, target).item()\n    metrics['diversity'] = compute_mind_diversity(mind_outputs)\n    metrics['convergence'] = compute_convergence(mind_thoughts)\n    return metrics\n\n# ============================================================================\n# DATA LOADING\n# ============================================================================\n\ndef load_data():\n    print_separator(\"LOADING DATA\")\n    \n    try:\n        df = pd.read_excel('/kaggle/input/chain-of-thoughts-for-arithmetic-prompts/cot_arithmatic_data.xlsx')\n        print(f\"  Loaded {len(df)} samples\")\n        \n        problems = df['prompt_format_one'].tolist()\n        answers = df['prompt_one_answer'].tolist()\n        \n        tokenizer = SimpleTokenizer(vocab_size=CONFIG['vocab_size'])\n        tokenizer.fit(problems)\n        \n        X, Y = [], []\n        for prob, ans in zip(problems, answers):\n            if pd.notna(prob) and pd.notna(ans):\n                X.append(tokenizer.encode(str(prob), CONFIG['max_seq_len']))\n                try:\n                    Y.append(float(ans))\n                except:\n                    Y.append(0.0)\n        \n        X = torch.tensor(X, dtype=torch.long)\n        Y = torch.tensor(Y, dtype=torch.float32)\n        Y = (Y - Y.mean()) / (Y.std() + 1e-8)\n        Y = Y.unsqueeze(1)\n        \n        n_train = min(CONFIG['train_samples'], int(len(X) * 0.8))\n        n_test = min(CONFIG['test_samples'], len(X) - n_train)\n        \n        print(f\"  Train: {n_train}, Test: {n_test}\")\n        \n        return (X[:n_train].to(CONFIG['device']), \n                Y[:n_train].to(CONFIG['device']),\n                X[n_train:n_train+n_test].to(CONFIG['device']), \n                Y[n_train:n_train+n_test].to(CONFIG['device']))\n        \n    except Exception as e:\n        print(f\"  Error: {e}, using synthetic data\")\n        total = CONFIG['train_samples'] + CONFIG['test_samples']\n        X = torch.randint(1, CONFIG['vocab_size'], (total, CONFIG['max_seq_len']))\n        Y = X.float().mean(dim=1, keepdim=True)\n        Y = (Y - Y.mean()) / Y.std()\n        \n        return (X[:CONFIG['train_samples']].to(CONFIG['device']),\n                Y[:CONFIG['train_samples']].to(CONFIG['device']),\n                X[CONFIG['train_samples']:].to(CONFIG['device']),\n                Y[CONFIG['train_samples']:].to(CONFIG['device']))\n\n# ============================================================================\n# TRAINING - FIXED\n# ============================================================================\n\ndef train_epoch(model, optimizer, X, Y):\n    model.train()\n    total_loss = 0\n    n_batches = 0\n    \n    for i in range(0, len(X), CONFIG['batch_size']):\n        batch_x = X[i:i+CONFIG['batch_size']]\n        batch_y = Y[i:i+CONFIG['batch_size']]\n        \n        if len(batch_x) < 2:\n            continue\n        \n        optimizer.zero_grad()\n        \n        pred, _ = model(batch_x, return_details=False)\n        loss = F.mse_loss(pred, batch_y)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        n_batches += 1\n    \n    return total_loss / max(n_batches, 1)\n\ndef evaluate(model, X, Y):\n    model.eval()\n    all_metrics = defaultdict(list)\n    \n    with torch.no_grad():\n        for i in range(0, len(X), CONFIG['batch_size']):\n            batch_x = X[i:i+CONFIG['batch_size']]\n            batch_y = Y[i:i+CONFIG['batch_size']]\n            \n            if len(batch_x) < 2:\n                continue\n            \n            if hasattr(model, 'layers') and hasattr(model.layers[0], 'attention'):\n                pred, _ = model(batch_x, return_details=False)\n                _, mind_outputs, mind_thoughts = model(batch_x, return_details=True)\n            else:\n                pred, thoughts = model(batch_x, return_details=False)\n                mind_outputs, mind_thoughts = None, None\n            \n            metrics = compute_metrics(pred, batch_y, mind_outputs, mind_thoughts)\n            for k, v in metrics.items():\n                all_metrics[k].append(v)\n    \n    return {k: np.mean(v) for k, v in all_metrics.items()}\n\n# ============================================================================\n# MAIN ARENA\n# ============================================================================\n\ndef run_scalable_arena():\n    print_separator(\"SCALABLE DIALECTICAL TRANSFORMER ARENA\", \"=\", 70)\n    print_config()\n    \n    X_train, Y_train, X_test, Y_test = load_data()\n    \n    print_separator(\"MODEL INITIALIZATION\")\n    models = {\n        'Standard': StandardTransformer(),\n        'Persona': PersonaTransformer(),\n        'Dialectical': DialecticalTransformer()\n    }\n    \n    for name, model in models.items():\n        model = model.to(CONFIG['device'])\n        params, _ = count_parameters(model)\n        print(f\"  {name}: {params:,} params\")\n    \n    all_results = {}\n    \n    for model_name, model in models.items():\n        print_separator(f\"TRAINING: {model_name}\", \"-\", 70)\n        \n        model = model.to(CONFIG['device'])\n        optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=0.01)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, CONFIG['num_epochs'])\n        \n        best_mse = float('inf')\n        history = []\n        \n        for epoch in range(CONFIG['num_epochs']):\n            start = time.time()\n            \n            train_loss = train_epoch(model, optimizer, X_train, Y_train)\n            test_metrics = evaluate(model, X_test, Y_test)\n            \n            scheduler.step()  # Step once per epoch, not per batch\n            \n            if test_metrics['mse'] < best_mse:\n                best_mse = test_metrics['mse']\n            \n            history.append({'epoch': epoch + 1, 'train_loss': train_loss, 'test': test_metrics})\n            \n            div = test_metrics.get('diversity', 0)\n            conv = test_metrics.get('convergence', 0)\n            conv_str = \"‚úì\" if conv > 0 else \"‚úó\"\n            \n            print(f\"  Epoch {epoch+1:2d} | Loss: {train_loss:.4f} | \"\n                  f\"MSE: {test_metrics['mse']:.4f} | \"\n                  f\"Div: {div:.3f} | Conv: {conv:.3f} {conv_str}\")\n        \n        all_results[model_name] = {\n            'best_mse': best_mse,\n            'history': history,\n            'final': test_metrics\n        }\n    \n    print_separator(\"FINAL RESULTS\", \"=\", 70)\n    \n    print(f\"\\n{'Model':<20} {'Best MSE':>12} {'Diversity':>12} {'Convergence':>12}\")\n    print(\"-\" * 60)\n    \n    for name, res in all_results.items():\n        div = res['final'].get('diversity', 0)\n        conv = res['final'].get('convergence', 0)\n        print(f\"{name:<20} {res['best_mse']:>12.6f} {div:>12.4f} {conv:>12.4f}\")\n    \n    print_separator(\"ANALYSIS\", \"-\", 70)\n    \n    best_model = min(all_results.keys(), key=lambda k: all_results[k]['best_mse'])\n    baseline_mse = all_results['Standard']['best_mse']\n    \n    print(f\"\\n  üèÜ WINNER: {best_model}\")\n    print(f\"     MSE: {all_results[best_model]['best_mse']:.6f}\")\n    \n    print(f\"\\n  Improvements over Standard Transformer:\")\n    for name, res in all_results.items():\n        if name != 'Standard':\n            imp = (baseline_mse - res['best_mse']) / baseline_mse * 100\n            print(f\"    {name}: {'+' if imp > 0 else ''}{imp:.1f}%\")\n    \n    print_separator(\"LEARNING CURVES\", \"-\", 70)\n    \n    for name, res in all_results.items():\n        mses = [h['test']['mse'] for h in res['history']]\n        min_mse, max_mse = min(mses), max(mses)\n        range_mse = max_mse - min_mse if max_mse > min_mse else 1\n        \n        print(f\"\\n  {name}:\")\n        for i, mse in enumerate(mses[:10]):  # Show first 10 epochs\n            norm = (mse - min_mse) / range_mse\n            bar_len = int((1 - norm) * 25)\n            bar = \"‚ñà\" * bar_len + \"‚ñë\" * (25 - bar_len)\n            print(f\"    E{i+1:2d}: {bar} {mse:.6f}\")\n    \n    print_separator(\"SCALING GUIDE\", \"=\", 70)\n    print(\"\"\"\n  üöÄ To scale this architecture to LLM size:\n  \n  Current (Proof of Concept):\n    d_model=256, n_layers=4, n_heads=8, n_minds=2\n    ~3.7M params\n  \n  Medium (GPT-2 scale):\n    d_model=768, n_layers=12, n_heads=12, n_minds=2\n    ~117M params\n  \n  Large (GPT-3 Small scale):\n    d_model=1024, n_layers=24, n_heads=16, n_minds=2\n    ~350M params\n  \n  XL (GPT-3 scale):\n    d_model=4096, n_layers=32, n_heads=32, n_minds=4\n    ~1.3B params\n  \n  üí° Key: Dialectical overhead is only ~10%\n     You get \"thinking\" almost for free!\n    \"\"\")\n    \n    print_separator(\"COMPLETE\", \"=\", 70)\n    \n    return all_results\n\n# ============================================================================\n# RUN\n# ============================================================================\n\nif __name__ == \"__main__\":\n    torch.manual_seed(42)\n    np.random.seed(42)\n    \n    results = run_scalable_arena()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-21T22:26:04.701034Z","iopub.execute_input":"2026-01-21T22:26:04.701707Z","iopub.status.idle":"2026-01-21T22:26:13.560100Z","shell.execute_reply.started":"2026-01-21T22:26:04.701681Z","shell.execute_reply":"2026-01-21T22:26:13.559540Z"}},"outputs":[{"name":"stdout","text":"\n=============== SCALABLE DIALECTICAL TRANSFORMER ARENA ===============\n\n============== SCALABLE DIALECTICAL TRANSFORMER CONFIG ==============\n  d_model                  : 256\n  n_heads                  : 8\n  n_layers                 : 4\n  n_minds                  : 2\n  d_ff                     : 512\n  dropout                  : 0.1\n  bottleneck_ratio         : 0.25\n  vocab_size               : 5000\n  max_seq_len              : 128\n  batch_size               : 8\n  num_epochs               : 20\n  learning_rate            : 0.0005\n  train_samples            : 600\n  test_samples             : 150\n  device                   : cuda\n======================================================================\n\n============================ LOADING DATA ============================\n  Loaded 100 samples\n  Train: 68, Test: 18\n\n======================== MODEL INITIALIZATION ========================\n  Standard: 3,487,745 params\n  Persona: 2,301,699 params\n  Dialectical: 3,716,353 params\n\n------------------------- TRAINING: Standard -------------------------\n  Epoch  1 | Loss: 1.3704 | MSE: 0.0010 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  2 | Loss: 1.1941 | MSE: 0.0099 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  3 | Loss: 1.1741 | MSE: 0.0067 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  4 | Loss: 1.1180 | MSE: 0.0059 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  5 | Loss: 1.1705 | MSE: 0.0027 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  6 | Loss: 1.0571 | MSE: 0.0098 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  7 | Loss: 0.8811 | MSE: 0.0094 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  8 | Loss: 0.6058 | MSE: 0.0044 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  9 | Loss: 0.4320 | MSE: 0.0009 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 10 | Loss: 0.2870 | MSE: 0.0003 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 11 | Loss: 0.1676 | MSE: 0.0004 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 12 | Loss: 0.1341 | MSE: 0.0004 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 13 | Loss: 0.0889 | MSE: 0.0004 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 14 | Loss: 0.0599 | MSE: 0.0004 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 15 | Loss: 0.0360 | MSE: 0.0004 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 16 | Loss: 0.0158 | MSE: 0.0003 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 17 | Loss: 0.0154 | MSE: 0.0004 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 18 | Loss: 0.0221 | MSE: 0.0003 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 19 | Loss: 0.0131 | MSE: 0.0003 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 20 | Loss: 0.0115 | MSE: 0.0003 | Div: 0.000 | Conv: 0.000 ‚úó\n\n------------------------- TRAINING: Persona -------------------------\n  Epoch  1 | Loss: 1.4123 | MSE: 0.0103 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  2 | Loss: 1.1761 | MSE: 0.0011 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  3 | Loss: 1.1870 | MSE: 0.0018 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  4 | Loss: 1.1911 | MSE: 0.0085 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  5 | Loss: 1.2100 | MSE: 0.0041 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  6 | Loss: 1.2098 | MSE: 0.0007 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  7 | Loss: 1.1873 | MSE: 0.0007 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  8 | Loss: 1.1809 | MSE: 0.0017 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch  9 | Loss: 1.1608 | MSE: 0.0059 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 10 | Loss: 1.1717 | MSE: 0.0011 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 11 | Loss: 1.1374 | MSE: 0.0036 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 12 | Loss: 1.0878 | MSE: 0.0023 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 13 | Loss: 1.0391 | MSE: 0.0030 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 14 | Loss: 0.9541 | MSE: 0.0078 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 15 | Loss: 0.8415 | MSE: 0.0105 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 16 | Loss: 0.7534 | MSE: 0.0032 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 17 | Loss: 0.7492 | MSE: 0.0051 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 18 | Loss: 0.7018 | MSE: 0.0040 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 19 | Loss: 0.6726 | MSE: 0.0043 | Div: 0.000 | Conv: 0.000 ‚úó\n  Epoch 20 | Loss: 0.6541 | MSE: 0.0045 | Div: 0.000 | Conv: 0.000 ‚úó\n\n----------------------- TRAINING: Dialectical -----------------------\n  Epoch  1 | Loss: 1.2910 | MSE: 0.0002 | Div: 1.049 | Conv: 0.125 ‚úì\n  Epoch  2 | Loss: 1.1996 | MSE: 0.0020 | Div: 1.002 | Conv: 0.204 ‚úì\n  Epoch  3 | Loss: 1.1886 | MSE: 0.0010 | Div: 0.963 | Conv: 0.234 ‚úì\n  Epoch  4 | Loss: 1.1536 | MSE: 0.0013 | Div: 0.951 | Conv: 0.200 ‚úì\n  Epoch  5 | Loss: 0.9765 | MSE: 0.0017 | Div: 0.964 | Conv: 0.092 ‚úì\n  Epoch  6 | Loss: 0.8469 | MSE: 0.0009 | Div: 0.956 | Conv: -0.055 ‚úó\n  Epoch  7 | Loss: 0.9249 | MSE: 0.0004 | Div: 0.953 | Conv: 0.007 ‚úì\n  Epoch  8 | Loss: 0.6739 | MSE: 0.0002 | Div: 0.951 | Conv: 0.022 ‚úì\n  Epoch  9 | Loss: 0.5805 | MSE: 0.0002 | Div: 0.950 | Conv: 0.013 ‚úì\n  Epoch 10 | Loss: 0.4544 | MSE: 0.0002 | Div: 0.949 | Conv: 0.005 ‚úì\n  Epoch 11 | Loss: 0.3860 | MSE: 0.0002 | Div: 0.949 | Conv: 0.002 ‚úì\n  Epoch 12 | Loss: 0.3503 | MSE: 0.0003 | Div: 0.949 | Conv: -0.007 ‚úó\n  Epoch 13 | Loss: 0.2910 | MSE: 0.0003 | Div: 0.949 | Conv: -0.012 ‚úó\n  Epoch 14 | Loss: 0.2425 | MSE: 0.0003 | Div: 0.948 | Conv: -0.012 ‚úó\n  Epoch 15 | Loss: 0.1725 | MSE: 0.0003 | Div: 0.948 | Conv: -0.012 ‚úó\n  Epoch 16 | Loss: 0.1610 | MSE: 0.0003 | Div: 0.948 | Conv: -0.014 ‚úó\n  Epoch 17 | Loss: 0.1731 | MSE: 0.0003 | Div: 0.948 | Conv: -0.015 ‚úó\n  Epoch 18 | Loss: 0.1617 | MSE: 0.0003 | Div: 0.948 | Conv: -0.016 ‚úó\n  Epoch 19 | Loss: 0.1299 | MSE: 0.0003 | Div: 0.948 | Conv: -0.017 ‚úó\n  Epoch 20 | Loss: 0.1567 | MSE: 0.0003 | Div: 0.948 | Conv: -0.017 ‚úó\n\n=========================== FINAL RESULTS ===========================\n\nModel                    Best MSE    Diversity  Convergence\n------------------------------------------------------------\nStandard                 0.000308       0.0000       0.0000\nPersona                  0.000663       0.0000       0.0000\nDialectical              0.000234       0.9478      -0.0170\n\n------------------------------ ANALYSIS ------------------------------\n\n  üèÜ WINNER: Dialectical\n     MSE: 0.000234\n\n  Improvements over Standard Transformer:\n    Persona: -115.6%\n    Dialectical: +24.0%\n\n-------------------------- LEARNING CURVES --------------------------\n\n  Standard:\n    E 1: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.000953\n    E 2: ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.009930\n    E 3: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.006669\n    E 4: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.005866\n    E 5: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.002687\n    E 6: ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.009754\n    E 7: ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.009362\n    E 8: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.004392\n    E 9: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.000885\n    E10: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.000337\n\n  Persona:\n    E 1: ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.010312\n    E 2: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.001061\n    E 3: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë 0.001797\n    E 4: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.008486\n    E 5: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.004141\n    E 6: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.000663\n    E 7: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.000699\n    E 8: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë 0.001654\n    E 9: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.005863\n    E10: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.001134\n\n  Dialectical:\n    E 1: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.000234\n    E 2: ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.002050\n    E 3: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.000992\n    E 4: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.001280\n    E 5: ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.001733\n    E 6: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.000887\n    E 7: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 0.000370\n    E 8: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.000245\n    E 9: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.000235\n    E10: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.000242\n\n=========================== SCALING GUIDE ===========================\n\n  üöÄ To scale this architecture to LLM size:\n  \n  Current (Proof of Concept):\n    d_model=256, n_layers=4, n_heads=8, n_minds=2\n    ~3.7M params\n  \n  Medium (GPT-2 scale):\n    d_model=768, n_layers=12, n_heads=12, n_minds=2\n    ~117M params\n  \n  Large (GPT-3 Small scale):\n    d_model=1024, n_layers=24, n_heads=16, n_minds=2\n    ~350M params\n  \n  XL (GPT-3 scale):\n    d_model=4096, n_layers=32, n_heads=32, n_minds=4\n    ~1.3B params\n  \n  üí° Key: Dialectical overhead is only ~10%\n     You get \"thinking\" almost for free!\n    \n\n============================== COMPLETE ==============================\n","output_type":"stream"}],"execution_count":9}]}